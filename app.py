# ==========================================
# ä¸‡éƒ¨çº§ WebDAV å½±è§†å¼•æ“ (ç»ˆæé˜²å¼¹ç‰ˆ)
# 1. è§£è¯´ç²‰ç¢æœºï¼šå­—é¢+M3U8çœŸå®æ—¶é•¿åŒé‡æ ¡éªŒï¼Œä½äº15åˆ†é’Ÿè‡ªåŠ¨æ¢æº
# 2. æ´—æµå¼•æ“ï¼šç²¾å‡†å‰”é™¤åˆ‡ç‰‡å¹¿å‘Š
# 3. æç®€åˆ†ç±»ï¼šæ”¯æŒ æœ€æ–°/çƒ­é—¨/é«˜åˆ† ç‹¬ç«‹æ–‡ä»¶å¤¹
# ==========================================

import os
import re
import urllib.parse
import concurrent.futures
from flask import Flask, request, Response, redirect
import requests
import urllib3
from collections import Counter

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
app = Flask(__name__)

# ==========================================
# âš™ï¸ æ ¸å¿ƒ API é…ç½®
# ==========================================
API_SOURCES = {
    "éå‡¡": "http://cj.ffzyapi.com/api.php/provide/vod/",
    "å§é¾™": "https://collect.wolongzyw.com/api.php/provide/vod/",
    "æœ€å¤§": "https://fapi.zuidapi.com/api.php/provide/vod/",
    "é»‘æœ¨è€³": "https://json.heimuer.xyz/api.php/provide/vod/",
    "æ— å°½": "https://api.wujinapi.me/api.php/provide/vod/",
    "ikun": "https://ikunzyapi.com/api.php/provide/vod/",
    "æ—¥å½±": "https://cj.rycjapi.com/api.php/provide/vod/",
    "FBèµ„æº": "https://fbzyapi.com/api.php/provide/vod/",
    "ç™¾åº¦": "https://api.apibdzy.com/api.php/provide/vod/"
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Referer': 'https://movie.douban.com/'
}

LIBRARY_CATEGORIES = {
    "ğŸ†• æœ€æ–°ä¸Šçº¿ç”µå½±": {"type": "movie", "tag": "æœ€æ–°", "sort": "time"},
    "ğŸ†• æœ€æ–°å¼€æ’­å‰§é›†": {"type": "tv", "tag": "æœ€æ–°", "sort": "time"},
    "ğŸ¬ çƒ­é—¨ç”µå½±åº“": {"type": "movie", "tag": "çƒ­é—¨", "sort": "recommend"},
    "ğŸ† é«˜åˆ†ç”µå½±æ¦œ": {"type": "movie", "tag": "è±†ç“£é«˜åˆ†", "sort": "recommend"},
    "ğŸ“º çƒ­é—¨ç”µè§†å‰§": {"type": "tv", "tag": "çƒ­é—¨", "sort": "recommend"},
    "ğŸŒ ç»å…¸çºªå½•ç‰‡": {"type": "tv", "tag": "çºªå½•ç‰‡", "sort": "recommend"}
}

DOUBAN_CHUNK_CACHE = {} 
TV_EPISODES_CACHE = {}

# ==========================================
# 1. M3U8 å¹¿å‘Šæ¸…æ´—ä¸ã€æ—¶é•¿æ£€æµ‹å¼•æ“ã€‘
# ==========================================

def get_m3u8_duration(m3u8_url):
    """æå– m3u8 æ€»æ—¶é•¿(ç§’)ï¼Œç”¨äºåˆ¤å®šæ˜¯å¦ä¸ºè§£è¯´/é¢„å‘Šç‰‡"""
    try:
        r = requests.get(m3u8_url, headers=HEADERS, timeout=5, verify=False)
        content = r.text
        # å¦‚æœæ˜¯ä¸»æ’­æ”¾åˆ—è¡¨ (åŒ…å«åˆ†è¾¨ç‡é€‰æ‹©)ï¼Œé€šå¸¸æ˜¯æ­£è§„å¤§ç‰‡ï¼Œç›´æ¥æ”¾è¡Œ
        if "RESOLUTION=" in content: return 9999 
        # ç´¯åŠ æ‰€æœ‰çš„åˆ‡ç‰‡æ—¶é•¿
        duration = sum(float(m) for m in re.findall(r'#EXTINF:([\d\.]+)', content))
        return duration
    except: return 0

def clean_m3u8_stream(m3u8_url):
    """å‰”é™¤å¹¿å‘Šåˆ‡ç‰‡"""
    try:
        r = requests.get(m3u8_url, headers=HEADERS, timeout=8, verify=False)
        content = r.text
        if "RESOLUTION=" in content:
            for line in content.splitlines():
                if line.endswith('.m3u8'):
                    if not line.startswith('http'): line = f"{m3u8_url.rsplit('/', 1)[0]}/{line}"
                    return clean_m3u8_stream(line)
                    
        lines = content.splitlines()
        clean_lines, ts_urls = [], []
        base_path = m3u8_url.rsplit('/', 1)[0]
        
        for line in lines:
            if not line.startswith('#') and line.strip():
                ts_urls.append(line if line.startswith('http') else f"{base_path}/{line}")
                
        if not ts_urls: return content
        
        domains = [urllib.parse.urlparse(url).netloc for url in ts_urls if url.startswith('http')]
        if not domains: return content
        main_domain = Counter(domains).most_common(1)[0][0]
        
        for line in lines:
            if line.startswith('#EXT-X-DISCONTINUITY'): continue
            if line.startswith('#EXTINF'):
                clean_lines.append(line)
                continue
                
            if not line.startswith('#') and line.strip():
                ts_url = line if line.startswith('http') else f"{base_path}/{line}"
                if urllib.parse.urlparse(ts_url).netloc != main_domain:
                    if clean_lines and clean_lines[-1].startswith('#EXTINF'): clean_lines.pop()
                    continue
                clean_lines.append(ts_url)
            else:
                clean_lines.append(line)
        return '\n'.join(clean_lines)
    except: return None

@app.route('/proxy/m3u8')
def proxy_m3u8():
    url = request.args.get('url')
    if not url: return "Missing URL", 400
    cleaned = clean_m3u8_stream(url)
    if cleaned: return Response(cleaned, mimetype='application/vnd.apple.mpegurl')
    return redirect(url, code=302)

# ==========================================
# 2. è±†ç“£å¼•æ“
# ==========================================
def fetch_douban_page(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=8)
        if r.status_code != 200: return []
        return [re.sub(r'[\\/*?:"<>|]', "", i.get('title', '')).strip() for i in r.json().get('subjects', [])]
    except: return []

def fetch_douban_chunk(tag, is_movie, offset=0, count=1000, sort_method="recommend"):
    cache_key = f"{tag}_{sort_method}_{offset}_{count}"
    if cache_key in DOUBAN_CHUNK_CACHE: return DOUBAN_CHUNK_CACHE[cache_key]
    t_type = "movie" if is_movie else "tv"
    urls = [f"https://movie.douban.com/j/search_subjects?type={t_type}&tag={urllib.parse.quote(tag)}&sort={sort_method}&page_limit=50&page_start={i}" for i in range(offset, offset + count, 50)]

    print(f"\n[*] ğŸŒŠ æ­£åœ¨æ‹‰å– è±†ç“£ {tag} ({sort_method}) ç¬¬ {offset+1}-{offset+count} éƒ¨...")
    results, seen = [], set()
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        for f in [executor.submit(fetch_douban_page, url) for url in urls]:
            for name in f.result():
                if name and name not in seen:
                    seen.add(name)
                    results.append(name)
    if not results: results = ["è±†ç“£æ¥å£é™åˆ¶_è¯·ç¨åå†è¯•"]
    DOUBAN_CHUNK_CACHE[cache_key] = results
    return results

# ==========================================
# 3. æœç´¢ä¸ã€è§£è¯´ç²‰ç¢æœºã€‘å¼•æ“
# ==========================================
def search_single_api(api_url, keyword):
    try:
        r = requests.get(f"{api_url}?ac=detail&wd={urllib.parse.quote(keyword)}", headers=HEADERS, timeout=6, verify=False)
        valid_vods = []
        for vod in r.json().get('list', []):
            name = vod.get('vod_name', '')
            t_name = str(vod.get('type_name', ''))
            # ã€è¿‡æ»¤ç¬¬ä¸€å±‚ã€‘ï¼šç»å¯¹ä¸è¦è§£è¯´ã€é€Ÿçœ‹ã€é¢„å‘Šç‰‡ã€çŸ­å‰§ï¼
            if any(x in name for x in ["è§£è¯´", "é€Ÿçœ‹", "é¢„å‘Š", "åˆ†é’Ÿ"]): continue
            if any(x in t_name for x in ["è§£è¯´", "çŸ­å‰§", "é¢„å‘Š"]): continue
            valid_vods.append(vod)
        return valid_vods
    except: return []

def get_movie_stream(keyword):
    print(f"\n[â–¶ å¯»å€] æ­£åœ¨å¹¶å‘æœå¯»: {keyword}")
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(search_single_api, url, keyword) for url in API_SOURCES.values()]
        for future in concurrent.futures.as_completed(futures):
            for vod in future.result():
                if keyword not in vod.get('vod_name', ''): continue
                play_url_str = vod.get('vod_play_url', '')
                for group in play_url_str.split('$$$'):
                    if '.m3u8' in group or '.mp4' in group:
                        for ep in group.split('#'):
                            ep_url = ep.split('$', 1)[1] if '$' in ep else ep
                            # ã€è¿‡æ»¤ç¬¬äºŒå±‚ã€‘ï¼šä¸¥æ ¼è®¡ç®—çœŸå®è§†é¢‘æ—¶é•¿ï¼Œå°‘äº 900ç§’(15åˆ†é’Ÿ) ç›´æ¥æŠ›å¼ƒï¼Œæ‰¾ä¸‹ä¸€ä¸ªï¼
                            if get_m3u8_duration(ep_url) >= 900:
                                return ep_url
                            else:
                                print(f" [!] æ‹¦æˆªåˆ°çŸ­è§†é¢‘/è§£è¯´æ¬ºè¯ˆ ({ep_url})ï¼Œè‡ªåŠ¨æ¢æºä¸­...")
    return None

def get_tv_episodes(keyword):
    print(f"\n[ğŸ“‚ å‰§é›†] æ­£åœ¨æ‹‰å–å…¨é›†: {keyword}")
    episodes_dict = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(search_single_api, url, keyword): name for name, url in API_SOURCES.items()}
        for future in concurrent.futures.as_completed(futures):
            source_name = futures[future]
            for vod in future.result():
                if keyword not in vod.get('vod_name', ''): continue
                play_url_str = vod.get('vod_play_url', '')
                for group in play_url_str.split('$$$'):
                    if '.m3u8' in group or '.mp4' in group:
                        for ep in group.split('#'):
                            if '$' in ep:
                                ep_name, ep_url = ep.split('$', 1)
                                safe_ep_name = re.sub(r'[\\/*?:"<>|]', "", ep_name).strip()
                                episodes_dict[f"[{source_name}] {keyword}_{safe_ep_name}.mp4"] = ep_url
                        return episodes_dict 
    return episodes_dict

# ==========================================
# 4. WebDAV è·¯ç”±æ ¸å¿ƒ
# ==========================================
def generate_propfind_xml(items):
    xml = ['<?xml version="1.0" encoding="utf-8" ?>', '<D:multistatus xmlns:D="DAV:">']
    for item in items:
        item_path = urllib.parse.quote(item['path'])
        xml.append('  <D:response>')
        xml.append(f'    <D:href>{item_path}</D:href>')
        xml.append('    <D:propstat><D:prop>')
        xml.append(f'      <D:displayname>{item["name"]}</D:displayname>')
        if item['is_dir']: xml.append('      <D:resourcetype><D:collection/></D:resourcetype>')
        else:
            xml.append('      <D:resourcetype/>')
            xml.append('      <D:getcontentlength>1073741824</D:getcontentlength>')
            xml.append('      <D:getcontenttype>video/mp4</D:getcontenttype>')
        xml.append('      <D:getlastmodified>Tue, 10 Jan 2024 12:00:00 GMT</D:getlastmodified>')
        xml.append('    </D:prop></D:propstat>')
        xml.append('    <D:status>HTTP/1.1 200 OK</D:status>')
        xml.append('  </D:response>')
    xml.append('</D:multistatus>')
    return '\n'.join(xml)

@app.route('/', defaults={'path': ''}, methods=['OPTIONS', 'PROPFIND', 'GET', 'HEAD'])
@app.route('/<path:path>', methods=['OPTIONS', 'PROPFIND', 'GET', 'HEAD'])
def webdav_handler(path):
    full_path = '/' + path if path else '/'
    decoded_path = urllib.parse.unquote(full_path).rstrip('/')
    parts = [p for p in decoded_path.split('/') if p]

    if request.method == 'OPTIONS':
        resp = Response()
        resp.headers['Allow'] = 'OPTIONS, PROPFIND, GET, HEAD'
        resp.headers['DAV'] = '1, 2'
        return resp

    if request.method == 'PROPFIND':
        items = []
        depth = request.headers.get('Depth', '1')

        if len(parts) == 0:
            items.append({'path': '/', 'name': 'Root', 'is_dir': True})
            if depth != '0':
                for cat in LIBRARY_CATEGORIES.keys(): items.append({'path': f"/{cat}", 'name': cat, 'is_dir': True})

        elif len(parts) == 1 and parts[0] in LIBRARY_CATEGORIES:
            items.append({'path': decoded_path, 'name': parts[0], 'is_dir': True})
            if depth != '0':
                items.extend([
                    {'path': f"{decoded_path}/ğŸ”¥ Top 1-1000 å¿…çœ‹ç²¾é€‰", 'name': 'ğŸ”¥ Top 1-1000 å¿…çœ‹ç²¾é€‰', 'is_dir': True},
                    {'path': f"{decoded_path}/ğŸ“š æµ©ç€šç‰‡åº“ (1000éƒ¨ä»¥å¤–)", 'name': 'ğŸ“š æµ©ç€šç‰‡åº“ (1000éƒ¨ä»¥å¤–)', 'is_dir': True}
                ])

        elif len(parts) == 2 and parts[0] in LIBRARY_CATEGORIES:
            items.append({'path': decoded_path, 'name': parts[1], 'is_dir': True})
            if depth != '0':
                cat_name = parts[0]
                is_movie = (LIBRARY_CATEGORIES[cat_name]['type'] == 'movie')
                tag = LIBRARY_CATEGORIES[cat_name]['tag']
                sort_method = LIBRARY_CATEGORIES[cat_name].get('sort', 'recommend')
                
                offset = 0 if "1-1000" in parts[1] else 1000
                names = fetch_douban_chunk(tag, is_movie, offset=offset, count=1000, sort_method=sort_method)
                
                for name in names:
                    if is_movie: 
                        items.append({'path': f"{decoded_path}/{name}.mp4", 'name': f"{name}.mp4", 'is_dir': False})
                    else: 
                        items.append({'path': f"{decoded_path}/{name}", 'name': name, 'is_dir': True})

        elif len(parts) == 3 and not parts[-1].endswith('.mp4'):
            tv_name = parts[-1]
            items.append({'path': decoded_path, 'name': tv_name, 'is_dir': True})
            if depth != '0':
                if tv_name not in TV_EPISODES_CACHE:
                    TV_EPISODES_CACHE[tv_name] = get_tv_episodes(tv_name)
                episodes = TV_EPISODES_CACHE[tv_name]
                if not episodes: items.append({'path': f"{decoded_path}/æœªæ‰¾åˆ°è¯¥å‰§æº.mp4", 'name': "æœªæ‰¾åˆ°è¯¥å‰§æº.mp4", 'is_dir': False})
                else:
                    for ep_name in episodes.keys(): items.append({'path': f"{decoded_path}/{ep_name}", 'name': ep_name, 'is_dir': False})

        elif decoded_path.endswith('.mp4'):
            items.append({'path': decoded_path, 'name': parts[-1], 'is_dir': False})
        else: return Response("Not Found", status=404)

        return Response(generate_propfind_xml(items), status=207, mimetype='application/xml')

    if request.method in ['GET', 'HEAD']:
        if decoded_path.endswith('.mp4'):
            if len(parts) == 3: 
                movie_name = parts[-1].replace('.mp4', '')
                m3u8_url = get_movie_stream(movie_name)
            else: 
                tv_name = parts[-2]
                ep_file_name = parts[-1]
                m3u8_url = TV_EPISODES_CACHE.get(tv_name, {}).get(ep_file_name)

            if m3u8_url:
                proxy_url = f"/proxy/m3u8?url={urllib.parse.quote(m3u8_url)}"
                print(f" -> ğŸ‰ æŠ•æ”¾çº¯å‡€èµ„æº: {m3u8_url}\n")
                return redirect(proxy_url, code=302) 

    return Response("Method Not Allowed", status=405)

if __name__ == '__main__':
    print("="*75)
    print(" ğŸŒ WebDAV å½±è§†ç»ˆæå¼•æ“ (é˜²å¼¹æ´—æµ+è§£è¯´è¿‡æ»¤ç‰ˆ) å¯åŠ¨å°±ç»ªï¼")
    print("="*75)
    app.run(host='0.0.0.0', port=8080, debug=False)