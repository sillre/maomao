# ==========================================
# ç‰ˆæœ¬: WebDAV å½±è§†èšåˆå¼•æ“ V7.3 (ç»ˆæå½¢æ€ç‰ˆ)
# 
# ã€å†å²ç‰ˆæœ¬å˜æ›´è®°å½•ã€‘
# - V6.9: ä¿®å¤å‰§é›†ç©ºè½½è¶…æ—¶é—®é¢˜ï¼Œå»¶æ—¶æé€Ÿå‹ç¼©ã€‚
# - V7.0: æ´—æµå¼•æ“å¼•å…¥ç›®å½•é¢‘æ¬¡æŒ‡çº¹ï¼Œå½»åº•ç‰©ç†åˆ é™¤å‹åˆ¶/åˆ‡ç‰‡å¹¿å‘Šã€‚
# - V7.1: æ–°å¢â€œæ— ç¼åˆ‡æºå¼¹å¤¹â€ã€‚
# - V7.2: å¼•å…¥â€œç”»è´¨æƒé‡æ‰“åˆ†å¼•æ“â€ï¼Œ4K/1080P/è“å…‰å¼ºåˆ¶æ’å‰ï¼Œæªç‰ˆ/TSå¼ºåˆ¶å«åº•ã€‚
#
# ã€V7.3 å½“å‰æ›´æ–°å†…å®¹ã€‘: 
#   1. å¼•å…¥â€œTime-Lock æ—¶é—´é”ä¸åŒå‡»åˆ‡æºâ€å¼•æ“ã€‚
#   2. è§£å†³ç»­æ’­ç—›ç‚¹ï¼šæ’­æ”¾è¶…è¿‡ 60 ç§’åï¼Œè‡ªåŠ¨å›ºåŒ–é”å®šå½“å‰æºç«™ï¼Œä¸‹æ¬¡ç»­æ’­ç»ä¸ä¹±åˆ‡ï¼Œä¿è¯è¿›åº¦å’Œç”»è´¨è¿è´¯ã€‚
#   3. æ“ä½œé©æ–°ï¼šè‹¥é‡å¡é¡¿éœ€è¦åˆ‡æºï¼Œåªéœ€â€œé€€å‡ºè§†é¢‘ï¼Œå¹¶åœ¨ 60 ç§’å†…å†æ¬¡ç‚¹å‡»æ’­æ”¾â€ï¼Œç³»ç»Ÿå³åˆ»ä¸ºæ‚¨åˆ‡æ¢ä¸‹ä¸€ä¸ªé¡¶çº§æºã€‚
# ==========================================

import os
import re
import urllib.parse
import concurrent.futures
import time
import random
import json
import threading
from flask import Flask, request, Response, redirect
import requests
import urllib3
from collections import Counter

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
app = Flask(__name__)

# ==========================================
# âš™ï¸ è¶…å¤§å¤‡èƒåº“
# ==========================================
MASTER_SOURCES = {
    "éå‡¡": "http://cj.ffzyapi.com/api.php/provide/vod/",
    "å§é¾™": "https://collect.wolongzyw.com/api.php/provide/vod/",
    "æœ€å¤§": "https://fapi.zuidapi.com/api.php/provide/vod/",
    "é»‘æœ¨è€³": "https://json.heimuer.xyz/api.php/provide/vod/",
    "æ— å°½": "https://api.wujinapi.me/api.php/provide/vod/",
    "ikun": "https://ikunzyapi.com/api.php/provide/vod/",
    "æ—¥å½±": "https://cj.rycjapi.com/api.php/provide/vod/",
    "FBèµ„æº": "https://fbzyapi.com/api.php/provide/vod/",
    "ç™¾åº¦": "https://api.apibdzy.com/api.php/provide/vod/",
    "é‡å­": "https://cj.lzyapi.com/api.php/provide/vod/",
    "å…‰é€Ÿ": "https://api.guangsuapi.com/api.php/provide/vod/",
    "çº¢ç‰›": "https://www.hongniuzy2.com/api.php/provide/vod/",
    "ç´¢å°¼": "https://suoniapi.com/api.php/provide/vod/",
    "å¿«è½¦": "https://caiji.kuaichezy.com/api.php/provide/vod/",
    "å¤©ç©º": "https://m3u8.tiankongapi.com/api.php/provide/vod/",
    "é£é€Ÿ": "https://www.feisuzyapi.com/api.php/provide/vod/",
    "Libvio": "https://www.libvio.com/api.php/provide/vod/",
    "æ–°å‚é•¿": "https://czzy.top/api.php/provide/vod/",
    "MonTV": "https://montv.api.com/api.php/provide/vod/",
    "é—ªç”µ": "https://sdzyapi.com/api.php/provide/vod/",
    "å¤©å ‚": "https://vip.kuaikan-api.com/api.php/provide/vod/",
    "æé€Ÿ": "https://jszyapi.com/api.php/provide/vod/",
    "è±ªå": "https://hhzyapi.com/api.php/provide/vod/",
    "é‡‘é¹°": "https://jyzyapi.com/api.php/provide/vod/"
}

ACTIVE_SOURCES = {}

LIBRARY_CATEGORIES = {
    "ğŸ”¥ å…¨ç½‘æºç«™å®æ—¶çƒ­æ¦œ (å…è±†ç“£)": {"type": "cms_hot"},
    "ğŸ†• æœ€æ–°ä¸Šçº¿ç”µå½±": {"type": "movie", "tag": "æœ€æ–°", "sort": "time"},
    "ğŸ†• æœ€æ–°å¼€æ’­å‰§é›†": {"type": "tv", "tag": "çƒ­é—¨", "sort": "time"},
    "ğŸ¬ çƒ­é—¨ç”µå½±æ€»æ¦œ": {"type": "movie", "tag": "çƒ­é—¨", "sort": "recommend"},
    "ğŸ† è±†ç“£é«˜åˆ†ç¥ä½œ": {"type": "movie", "tag": "è±†ç“£é«˜åˆ†", "sort": "recommend"},
    "ğŸ›¸ ç§‘å¹»å·¨åˆ¶ç²¾é€‰": {"type": "movie", "tag": "ç§‘å¹»", "sort": "recommend"},
    "ğŸ”« åŠ¨ä½œçˆ†ç±³èŠ±æ¦œ": {"type": "movie", "tag": "åŠ¨ä½œ", "sort": "recommend"},
    "ğŸ˜‚ å–œå‰§æ¬¢ä¹ç²¾é€‰": {"type": "movie", "tag": "å–œå‰§", "sort": "recommend"},
    "ğŸ‘» æƒŠæ‚šæ‚¬ç–‘çƒ§è„‘": {"type": "movie", "tag": "æ‚¬ç–‘", "sort": "recommend"},
    "ğŸ“º çƒ­é—¨å›½å‰§å¤§å…¨": {"type": "tv", "tag": "å›½äº§å‰§", "sort": "recommend"},
    "ğŸ“º ç»å…¸ç¾å‰§å¤§ç‰‡": {"type": "tv", "tag": "ç¾å‰§", "sort": "recommend"},
    "ğŸŒ¸ é«˜åˆ†åŠ¨æ¼«æ¼«åŒº": {"type": "tv", "tag": "æ—¥æœ¬åŠ¨ç”»", "sort": "recommend"},
    "ğŸŒ ç»å…¸çºªå½•ç‰‡åº“": {"type": "tv", "tag": "çºªå½•ç‰‡", "sort": "recommend"}
}

DOUBAN_LOCK = threading.Lock()
CACHE_FILE = "douban_cache_v6.json" 
DOUBAN_CACHE = {}

TV_EPISODES_CACHE = {}
MOVIE_STREAM_CACHE = {}

if os.path.exists(CACHE_FILE):
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            DOUBAN_CACHE = json.load(f)
    except: pass

def save_cache():
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(DOUBAN_CACHE, f, ensure_ascii=False, indent=2)
    except: pass

def get_random_ua():
    return random.choice([
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/119.0.0.0 Safari/537.36"
    ])

# ==========================================
# ğŸš€ æ¢é’ˆå¼•æ“
# ==========================================
def probe_single_source(name, url):
    try:
        start_time = time.time()
        r = requests.get(f"{url}?ac=list", headers={'User-Agent': get_random_ua()}, timeout=3, verify=False)
        if r.status_code == 200 and 'list' in r.json():
            return name, url, time.time() - start_time
    except: pass
    return name, None, 999

def update_active_sources():
    global ACTIVE_SOURCES
    print("\n[*] ğŸ•µï¸â€â™‚ï¸ æ¢é’ˆå¼•æ“å¯åŠ¨: æ­£åœ¨å…¨ç½‘æ‰«æå­˜æ´»æºç«™...")
    temp_sources = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        futures = [executor.submit(probe_single_source, name, url) for name, url in MASTER_SOURCES.items()]
        for future in concurrent.futures.as_completed(futures):
            name, valid_url, ping = future.result()
            if valid_url: temp_sources[name] = valid_url
                
    if temp_sources:
        ACTIVE_SOURCES = temp_sources
        print(f"[*] âœ… æ¢é’ˆæŠ¥å‘Š: æˆåŠŸç­›é€‰å‡º {len(ACTIVE_SOURCES)} ä¸ªé«˜é€Ÿå­˜æ´»èŠ‚ç‚¹ï¼")
    else:
        ACTIVE_SOURCES = {"é»˜è®¤å…œåº•": "http://cj.ffzyapi.com/api.php/provide/vod/"}

def background_probe_task():
    while True:
        time.sleep(43200)
        update_active_sources()

update_active_sources()
threading.Thread(target=background_probe_task, daemon=True).start()

# ==========================================
# 1. æ•°æ®è·å–å¼•æ“ 
# ==========================================
def fetch_cms_hot_list():
    cache_key = "cms_global_hot_v5" 
    if cache_key in DOUBAN_CACHE: return DOUBAN_CACHE[cache_key]
    
    results = set()
    def fetch_single_cms(url):
        for page in range(1, 6):
            try:
                r = requests.get(f"{url}?ac=detail&pg={page}", headers={'User-Agent': get_random_ua()}, timeout=5, verify=False)
                if r.status_code == 200:
                    v_list = r.json().get('list', [])
                    if not v_list: break 
                    
                    for vod in v_list:
                        name = re.sub(r'[\\/*?:"<>|]', "", vod.get('vod_name', '')).strip()
                        t_name = str(vod.get('type_name', ''))
                        
                        black_keywords = ["è§£è¯´", "é€Ÿçœ‹", "é¢„å‘Š", "åˆ†é’Ÿ", "çŸ­å‰§", "èŠ±çµ®", "ç›˜ç‚¹", "åˆé›†", 
                                          "ç¦åˆ©", "ä¼¦ç†", "ç»¼è‰º", "è®°å½•", "çºªå½•", "åŠ¨æ¼«", "åŠ¨ç”»", "ä½“è‚²", 
                                          "éŸ³ä¹", "å…¶ä»–", "å†™çœŸ", "å¾®ç”µ", "ç›²ç›’", "å¤´æ¡", "Bç«™", "æŠ–éŸ³", "å¿«æ‰‹"]
                        if any(x in name for x in black_keywords) or any(x in t_name for x in black_keywords): 
                            continue
                            
                        white_type_keywords = ["ç‰‡", "ç”µå½±", "å‰§", "è¿ç»­å‰§", "TV"]
                        if not any(x in t_name for x in white_type_keywords):
                            continue 
                            
                        results.add(name)
            except: pass

    print(f"\n[*] ğŸš€ æ­£åœ¨ç©¿é€ {len(ACTIVE_SOURCES)} ä¸ªå­˜æ´»æºç«™ï¼Œå‘ä¸‹æ·±æŒ– 5 é¡µè¿›è¡Œå…¨ç½‘æµ·é€‰...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(ACTIVE_SOURCES)) as executor:
        [executor.submit(fetch_single_cms, url) for url in ACTIVE_SOURCES.values()]
    
    final_list = list(results)
    if len(final_list) > 800: final_list = final_list[:800]
        
    if final_list:
        DOUBAN_CACHE[cache_key] = final_list
        save_cache()
        
    print(f"[*] âœ… å…è±†ç“£çƒ­æ¦œæå–å®Œæ¯•ï¼æœ€ç»ˆæ–©è· {len(final_list)} éƒ¨çº¯å‡€å¤§ç‰‡/çƒ­å‰§ï¼")
    return final_list

def fetch_douban_chunk(tag, is_movie, offset=0, count=250, sort_method="recommend"):
    t_type = "movie" if is_movie else "tv"
    cache_key = f"{t_type}_{tag}_{sort_method}_{offset}_{count}"
    
    if cache_key in DOUBAN_CACHE: return DOUBAN_CACHE[cache_key]

    with DOUBAN_LOCK:
        if cache_key in DOUBAN_CACHE: return DOUBAN_CACHE[cache_key]
        
        urls = [f"https://movie.douban.com/j/search_subjects?type={t_type}&tag={urllib.parse.quote(tag)}&sort={sort_method}&page_limit=50&page_start={i}" for i in range(offset, offset + count, 50)]
        results, seen = [], set()
        
        for url in urls:
            try:
                time.sleep(random.uniform(0.4, 0.8))
                r = requests.get(url, headers={'User-Agent': get_random_ua(), 'Referer': 'https://movie.douban.com/'}, timeout=8)
                if r.status_code == 403: return ["è±†ç“£æ¥å£é™åˆ¶_è¯·ä½¿ç”¨å…è±†ç“£æˆ–ç¨ååˆ·æ–°"]
                if r.status_code == 200:
                    for i in r.json().get('subjects', []):
                        name = re.sub(r'[\\/*?:"<>|]', "", i.get('title', '')).strip()
                        if name and name not in seen:
                            seen.add(name); results.append(name)
            except: pass
            
        if results:
            DOUBAN_CACHE[cache_key] = results
            save_cache()
            return results
        return ["è¯¥åˆ†ç±»è±†ç“£æš‚æ— æ•°æ®_æˆ–æ¥å£å¼‚å¸¸"]

# ==========================================
# 2. æ´—æµä¸æœç´¢å¼•æ“ 
# ==========================================
def get_video_quality_score(url, vod_name=""):
    text = (url + " " + vod_name).lower()
    score = 10 
    if '4k' in text or '2160' in text: score += 40
    elif '1080' in text: score += 30
    elif 'è“å…‰' in text or 'bd' in text: score += 25
    elif '720' in text: score += 20
    elif 'hd' in text or 'è¶…æ¸…' in text or 'é«˜æ¸…' in text: score += 15
    if 'ts' in text or 'tc' in text or 'æªç‰ˆ' in text or 'æŠ¢å…ˆ' in text or 'éŸ©ç‰ˆ' in text:
        score -= 50
    return score

def clean_m3u8_stream(m3u8_url):
    try:
        r = requests.get(m3u8_url, headers={'User-Agent': get_random_ua()}, timeout=8, verify=False)
        content = r.text
        if "RESOLUTION=" in content:
            for line in content.splitlines():
                if line.endswith('.m3u8'):
                    return clean_m3u8_stream(line if line.startswith('http') else f"{m3u8_url.rsplit('/', 1)[0]}/{line}")
                    
        lines = content.splitlines()
        base_path = m3u8_url.rsplit('/', 1)[0]
        ts_urls = [line if line.startswith('http') else f"{base_path}/{line}" for line in lines if not line.startswith('#') and line.strip()]
        if not ts_urls: return content
        
        dir_paths = [u.rsplit('/', 1)[0] for u in ts_urls]
        dir_counts = Counter(dir_paths)
        valid_dirs = {d for d, c in dir_counts.items() if c > (len(ts_urls) * 0.02) or c > 10}
        if not valid_dirs: valid_dirs = {dir_counts.most_common(1)[0][0]}
        
        clean_lines, has_vod_tag, i = [], False, 0
        while i < len(lines):
            line = lines[i].strip()
            if not line: i += 1; continue
            if line.startswith('#EXT-X-PLAYLIST-TYPE'):
                has_vod_tag = True; clean_lines.append(line)
            elif line.startswith('#EXTINF'):
                extinf_line = line
                i += 1
                if i < len(lines):
                    ts_url = lines[i].strip()
                    ts_url = ts_url if ts_url.startswith('http') else f"{base_path}/{ts_url}"
                    if ts_url.rsplit('/', 1)[0] in valid_dirs:
                        clean_lines.extend([extinf_line, ts_url])
            elif line.startswith('#EXTM3U') or line.startswith('#EXT-X-VERSION') or line.startswith('#EXT-X-TARGETDURATION') or line.startswith('#EXT-X-MEDIA-SEQUENCE'):
                clean_lines.append(line)
            i += 1

        if not has_vod_tag and len(clean_lines) > 1: clean_lines.insert(1, "#EXT-X-PLAYLIST-TYPE:VOD")
        clean_lines.append("#EXT-X-ENDLIST")
        return '\n'.join(clean_lines)
    except: return None

def check_playability_and_duration(m3u8_url):
    try:
        r = requests.get(m3u8_url, headers={'User-Agent': get_random_ua()}, timeout=5, verify=False)
        if r.status_code != 200: return False
        content = r.text
        if "RESOLUTION=" in content: return True
        return sum(float(m) for m in re.findall(r'#EXTINF:([\d\.]+)', content)) >= 1800
    except: return False

@app.route('/proxy/m3u8')
def proxy_m3u8():
    url = request.args.get('url')
    cleaned = clean_m3u8_stream(url)
    if cleaned: return Response(cleaned, mimetype='application/vnd.apple.mpegurl')
    return redirect(url, code=302)

def search_single_api(api_url, keyword):
    try:
        r = requests.get(f"{api_url}?ac=detail&wd={urllib.parse.quote(keyword)}", headers={'User-Agent': get_random_ua()}, timeout=6, verify=False)
        valid_vods = []
        for vod in r.json().get('list', []):
            name, t_name = vod.get('vod_name', ''), str(vod.get('type_name', ''))
            black_keywords = ["è§£è¯´", "é€Ÿçœ‹", "é¢„å‘Š", "åˆ†é’Ÿ", "çŸ­å‰§", "èŠ±çµ®", "ç›˜ç‚¹", "åˆé›†", "ç¦åˆ©", "ä¼¦ç†", "ç»¼è‰º", "è®°å½•", "åŠ¨æ¼«"]
            if any(x in name for x in black_keywords) or any(x in t_name for x in black_keywords): continue
            valid_vods.append(vod)
        return valid_vods
    except: return []

def get_movie_stream(keyword):
    if keyword not in MOVIE_STREAM_CACHE:
        seen_urls = set()
        url_scores = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(ACTIVE_SOURCES)) as executor:
            futures = [executor.submit(search_single_api, url, keyword) for url in ACTIVE_SOURCES.values()]
            for future in concurrent.futures.as_completed(futures):
                for vod in future.result():
                    if keyword not in vod.get('vod_name', ''): continue
                    for group in vod.get('vod_play_url', '').split('$$$'):
                        if '.m3u8' in group or '.mp4' in group:
                            for ep in group.split('#'):
                                ep_url = ep.split('$', 1)[1] if '$' in ep else ep
                                if ep_url not in seen_urls:
                                    seen_urls.add(ep_url)
                                    score = get_video_quality_score(ep_url, vod.get('vod_name', ''))
                                    url_scores.append((score, ep_url))
        
        if url_scores:
            url_scores.sort(key=lambda x: x[0], reverse=True)
            sorted_urls = [u for s, u in url_scores]
            MOVIE_STREAM_CACHE[keyword] = {"urls": sorted_urls, "index": 0, "last_time": 0}
            
    return MOVIE_STREAM_CACHE.get(keyword)

def get_tv_episodes(keyword):
    episodes_dict = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(ACTIVE_SOURCES)) as executor:
        futures = {executor.submit(search_single_api, url, keyword): name for name, url in ACTIVE_SOURCES.items()}
        for future in concurrent.futures.as_completed(futures):
            for vod in future.result():
                if keyword not in vod.get('vod_name', ''): continue
                for group in vod.get('vod_play_url', '').split('$$$'):
                    if '.m3u8' in group or '.mp4' in group:
                        for ep in group.split('#'):
                            if '$' in ep:
                                ep_name, ep_url = ep.split('$', 1)
                                nums = re.findall(r'\d+', ep_name)
                                # å½»åº•ä¿®å¤ Python 3.9 çš„ f-string æŠ¥é”™
                                safe_ep_name = re.sub(r'[\\/*?:"<>|]', "", ep_name).strip()
                                std_name = f"ç¬¬{nums[-1].zfill(2)}é›†.mp4" if nums else f"{safe_ep_name}.mp4"
                                
                                if std_name not in episodes_dict:
                                    episodes_dict[std_name] = {"url_scores": [], "seen_urls": set(), "index": 0, "last_time": 0}
                                
                                if ep_url not in episodes_dict[std_name]["seen_urls"]:
                                    episodes_dict[std_name]["seen_urls"].add(ep_url)
                                    score = get_video_quality_score(ep_url, vod.get('vod_name', ''))
                                    episodes_dict[std_name]["url_scores"].append((score, ep_url))

    for std_name in episodes_dict:
        episodes_dict[std_name]["url_scores"].sort(key=lambda x: x[0], reverse=True)
        episodes_dict[std_name]["urls"] = [u for s, u in episodes_dict[std_name]["url_scores"]]
        del episodes_dict[std_name]["url_scores"]
        del episodes_dict[std_name]["seen_urls"]
        
    return episodes_dict

# ==========================================
# 3. WebDAV è·¯ç”±
# ==========================================
def generate_propfind_xml(items):
    xml = ['<?xml version="1.0" encoding="utf-8" ?>', '<D:multistatus xmlns:D="DAV:">']
    for item in items:
        item_path = urllib.parse.quote(item['path'])
        xml.append(f'  <D:response>\n    <D:href>{item_path}</D:href>\n    <D:propstat><D:prop>')
        xml.append(f'      <D:displayname>{item["name"]}</D:displayname>')
        if item['is_dir']: xml.append('      <D:resourcetype><D:collection/></D:resourcetype>')
        else:
            xml.append('      <D:resourcetype/>\n      <D:getcontentlength>1073741824</D:getcontentlength>\n      <D:getcontenttype>video/mp4</D:getcontenttype>')
        xml.append('      <D:getlastmodified>Tue, 10 Jan 2024 12:00:00 GMT</D:getlastmodified>\n    </D:prop></D:propstat>\n    <D:status>HTTP/1.1 200 OK</D:status>\n  </D:response>')
    xml.append('</D:multistatus>')
    return '\n'.join(xml)

@app.route('/', defaults={'path': ''}, methods=['OPTIONS', 'PROPFIND', 'GET', 'HEAD'])
@app.route('/<path:path>', methods=['OPTIONS', 'PROPFIND', 'GET', 'HEAD'])
def webdav_handler(path):
    full_path = '/' + path if path else '/'
    decoded_path = urllib.parse.unquote(full_path).rstrip('/')
    parts = [p for p in decoded_path.split('/') if p]

    if request.method == 'OPTIONS':
        resp = Response()
        resp.headers['Allow'] = 'OPTIONS, PROPFIND, GET, HEAD'
        resp.headers['DAV'] = '1, 2'
        return resp

    if request.method == 'PROPFIND':
        items = []
        depth = request.headers.get('Depth', '1')

        if len(parts) == 0:
            items.append({'path': '/', 'name': 'Root', 'is_dir': True})
            if depth != '0':
                for cat in LIBRARY_CATEGORIES.keys(): items.append({'path': f"/{cat}", 'name': cat, 'is_dir': True})

        elif len(parts) == 1 and parts[0] in LIBRARY_CATEGORIES:
            items.append({'path': decoded_path, 'name': parts[0], 'is_dir': True})
            if depth != '0':
                cat_config = LIBRARY_CATEGORIES[parts[0]]
                if cat_config['type'] == 'cms_hot':
                    for name in fetch_cms_hot_list(): items.append({'path': f"{decoded_path}/{name}", 'name': name, 'is_dir': True})
                else:
                    for i in range(10):
                        start_idx = i * 250 + 1
                        end_idx = (i + 1) * 250
                        prefix = "ğŸ”¥" if i == 0 else "ğŸ“š"
                        folder_name = f"{prefix} Top {start_idx}-{end_idx}"
                        items.append({'path': f"{decoded_path}/{folder_name}", 'name': folder_name, 'is_dir': True})

        elif len(parts) == 2 and parts[0] in LIBRARY_CATEGORIES:
            items.append({'path': decoded_path, 'name': parts[1], 'is_dir': True})
            if depth != '0':
                cat_config = LIBRARY_CATEGORIES[parts[0]]
                if cat_config['type'] == 'cms_hot':
                    name = parts[1]
                    if name not in TV_EPISODES_CACHE: TV_EPISODES_CACHE[name] = get_tv_episodes(name)
                    episodes = TV_EPISODES_CACHE[name]
                    if not episodes: items.append({'path': f"{decoded_path}/æœªæ‰¾åˆ°æœ‰æ•ˆæº.mp4", 'name': "æœªæ‰¾åˆ°æœ‰æ•ˆæº.mp4", 'is_dir': False})
                    else:
                        for ep_name in episodes.keys(): items.append({'path': f"{decoded_path}/{ep_name}", 'name': ep_name, 'is_dir': False})
                else:
                    match = re.search(r'Top (\d+)-', parts[1])
                    offset = int(match.group(1)) - 1 if match else 0
                    
                    for name in fetch_douban_chunk(cat_config['tag'], (cat_config['type'] == 'movie'), offset=offset, count=250, sort_method=cat_config['sort']):
                        if cat_config['type'] == 'movie': items.append({'path': f"{decoded_path}/{name}.mp4", 'name': f"{name}.mp4", 'is_dir': False})
                        else: items.append({'path': f"{decoded_path}/{name}", 'name': name, 'is_dir': True})

        elif len(parts) == 3:
            if decoded_path.endswith('.mp4'):
                items.append({'path': decoded_path, 'name': parts[-1], 'is_dir': False})
            else:
                tv_name = parts[-1]
                items.append({'path': decoded_path, 'name': tv_name, 'is_dir': True})
                if depth != '0':
                    if tv_name not in TV_EPISODES_CACHE: TV_EPISODES_CACHE[tv_name] = get_tv_episodes(tv_name)
                    episodes = TV_EPISODES_CACHE[tv_name]
                    if not episodes: items.append({'path': f"{decoded_path}/æœªæ‰¾åˆ°è¯¥æºæˆ–æ—¶é•¿è¿‡çŸ­.mp4", 'name': "æœªæ‰¾åˆ°è¯¥æºæˆ–æ—¶é•¿è¿‡çŸ­.mp4", 'is_dir': False})
                    else:
                        for ep_name in sorted(episodes.keys()): items.append({'path': f"{decoded_path}/{ep_name}", 'name': ep_name, 'is_dir': False})

        elif len(parts) == 4 and decoded_path.endswith('.mp4'):
            items.append({'path': decoded_path, 'name': parts[-1], 'is_dir': False})
        else: return Response("Not Found", status=404)

        return Response(generate_propfind_xml(items), status=207, mimetype='application/xml')

    # ã€V7.3 æ ¸å¿ƒï¼šTime-Lock å›ºåŒ–ç»­æ’­ä¸åŒå‡»åˆ‡æºã€‘
    if request.method in ['GET', 'HEAD']:
        if decoded_path.endswith('.mp4'):
            parent_dir = parts[-2]
            file_name = parts[-1]
            m3u8_url = None
            
            ep_data = TV_EPISODES_CACHE.get(parent_dir, {}).get(file_name)
            if not ep_data:
                movie_name = file_name.replace('.mp4', '')
                ep_data = get_movie_stream(movie_name)
                
            if ep_data and ep_data.get("urls"):
                urls = ep_data["urls"]
                last_time = ep_data.get("last_time", 0)
                now = time.time()
                
                # åˆ¤æ–­ç”¨æˆ·æ˜¯åœ¨â€œç»­æ’­â€è¿˜æ˜¯åœ¨â€œè¯·æ±‚åˆ‡æºâ€
                # å¦‚æœç”¨æˆ·è·ç¦»ä¸Šæ¬¡ç‚¹å‡»ä¸è¶³ 60 ç§’ï¼Œè¯´æ˜é‡åˆ°äº†å¡é¡¿åœ¨å¼ºåˆ¶åˆ‡æº
                is_switching = (last_time > 0 and (now - last_time) < 60)
                
                # å¦‚æœåˆ‡æºï¼Œä»ä¸‹ä¸€ä¸ªèŠ‚ç‚¹å¼€å§‹æ‰¾ï¼›å¦‚æœæ˜¯ç»­æ’­ï¼Œä»è®°å½•çš„ç¨³å›ºèŠ‚ç‚¹æ‰¾
                check_start = (ep_data["index"] + 1) % len(urls) if is_switching else ep_data["index"]
                
                for i in range(len(urls)):
                    curr_idx = (check_start + i) % len(urls)
                    test_url = urls[curr_idx]
                    
                    if check_playability_and_duration(test_url):
                        ep_data["index"] = curr_idx   # å›ºåŒ–ç¨³å›ºæº
                        ep_data["last_time"] = now    # æ›´æ–°æ’­æ”¾æ—¶é—´é”
                        m3u8_url = test_url
                        break
                        
            if m3u8_url: return redirect(f"/proxy/m3u8?url={urllib.parse.quote(m3u8_url)}", code=302) 
            return Response("æ‰€æœ‰èŠ‚ç‚¹å‡ä¸å¯ç”¨æˆ–å·²è¢«è¿‡æ»¤", status=404)

    return Response("Method Not Allowed", status=405)

if __name__ == '__main__':
    print("="*75)
    print(f" ğŸŒ WebDAV å½±è§†ç»ˆæå¼•æ“ V7.3 (å›ºåŒ–æºç«™ä¸ç”»è´¨ä¼˜é€‰ç‰ˆ) å¯åŠ¨å°±ç»ªï¼")
    print("="*75)
    app.run(host='0.0.0.0', port=8080, debug=False)
