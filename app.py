# ==========================================
# ç‰ˆæœ¬: WebDAV å½±è§†èšåˆå¼•æ“ V6.9 (æœ€ç»ˆå®Œå–„ç‰ˆ)
# 
# ã€å†å²ç‰ˆæœ¬å˜æ›´è®°å½•ã€‘
# - V6.0: å¼•å…¥ AI è‡ªåŠ¨æ¢é’ˆï¼Œè‡ªåŠ¨å‰”é™¤æ­»é“¾ã€è¶…æ—¶ã€å¤±æ•ˆåŸŸåã€‚
# - V6.5: è§£é™¤ 500 éƒ¨é™åˆ¶ï¼Œé‡‡ç”¨æ·±åº¦åˆ†é¡µæ¶æ„ï¼Œæ€»å®¹é‡æ‰©å……è‡³è¿‘ 30,000 éƒ¨ã€‚
# - V6.6: å¼•å…¥â€œé»‘ç™½åŒåå•â€æœºåˆ¶ï¼Œåªå…è®¸åˆ†ç±»å¸¦â€œç‰‡/å‰§/ç”µå½±â€çš„èµ„æºæ”¾è¡Œï¼Œå½»åº•ç»æ€ç»¼è‰ºå’ŒçŸ­å‰§ã€‚
# - V6.7: â€œå…è±†ç“£çƒ­æ¦œâ€å¼•å…¥ pg æ·±åº¦åˆ†é¡µæŠ“å–ï¼Œå®¹é‡ä»å‡ åéƒ¨æš´å¢è‡³è¿‘åƒéƒ¨ã€‚
# - V6.8: ä¿®å¤è±†ç“£ç¼“å­˜é”®å€¼é—æ¼ t_type å¯¼è‡´â€œæœ€æ–°ç”µå½±â€ä¸â€œæœ€æ–°å‰§é›†â€æ··æ·†ç¢°æ’çš„ Bugã€‚
#
# ã€V6.9 å½“å‰æ›´æ–°å†…å®¹ã€‘: 
#   1. ä¿®å¤â€œæœ€æ–°å¼€æ’­å‰§é›†â€æ— æ³•è·å–çš„é—®é¢˜ï¼šè±†ç“£ TV ç±»æ— â€œæœ€æ–°â€æ ‡ç­¾ï¼Œä¿®æ­£ä¸ºâ€œçƒ­é—¨â€+ sort=timeã€‚
#   2. ä¼˜åŒ– PROPFIND è¶…æ—¶ï¼šå°†æŠ“å–å»¶æ—¶ä» 2.5s å‹ç¼©è‡³ 0.4-0.8sï¼Œé˜²æ­¢ VidHub ç­‰å¾…è¶… 10s æ–­å¼€è¿æ¥ã€‚
#   3. å‡çº§ä¸»ç¼“å­˜æ–‡ä»¶åä¸º douban_cache_v6.jsonï¼Œå¼ºè¡Œæ¸…ç©ºæ—§çš„ç©ºè½½ç¼“å­˜ã€‚
# ==========================================

import os
import re
import urllib.parse
import concurrent.futures
import time
import random
import json
import threading
from flask import Flask, request, Response, redirect
import requests
import urllib3
from collections import Counter

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
app = Flask(__name__)

# ==========================================
# âš™ï¸ è¶…å¤§å¤‡èƒåº“ (è‡ªåŠ¨æµ‹è¯•ï¼Œæ­»çš„ä¸¢å¼ƒï¼Œæ´»çš„é€‰ç”¨)
# ==========================================
MASTER_SOURCES = {
    "éå‡¡": "http://cj.ffzyapi.com/api.php/provide/vod/",
    "å§é¾™": "https://collect.wolongzyw.com/api.php/provide/vod/",
    "æœ€å¤§": "https://fapi.zuidapi.com/api.php/provide/vod/",
    "é»‘æœ¨è€³": "https://json.heimuer.xyz/api.php/provide/vod/",
    "æ— å°½": "https://api.wujinapi.me/api.php/provide/vod/",
    "ikun": "https://ikunzyapi.com/api.php/provide/vod/",
    "æ—¥å½±": "https://cj.rycjapi.com/api.php/provide/vod/",
    "FBèµ„æº": "https://fbzyapi.com/api.php/provide/vod/",
    "ç™¾åº¦": "https://api.apibdzy.com/api.php/provide/vod/",
    "é‡å­": "https://cj.lzyapi.com/api.php/provide/vod/",
    "å…‰é€Ÿ": "https://api.guangsuapi.com/api.php/provide/vod/",
    "çº¢ç‰›": "https://www.hongniuzy2.com/api.php/provide/vod/",
    "ç´¢å°¼": "https://suoniapi.com/api.php/provide/vod/",
    "å¿«è½¦": "https://caiji.kuaichezy.com/api.php/provide/vod/",
    "å¤©ç©º": "https://m3u8.tiankongapi.com/api.php/provide/vod/",
    "é£é€Ÿ": "https://www.feisuzyapi.com/api.php/provide/vod/",
    "Libvio": "https://www.libvio.com/api.php/provide/vod/",
    "æ–°å‚é•¿": "https://czzy.top/api.php/provide/vod/",
    "MonTV": "https://montv.api.com/api.php/provide/vod/",
    "é—ªç”µ": "https://sdzyapi.com/api.php/provide/vod/",
    "å¤©å ‚": "https://vip.kuaikan-api.com/api.php/provide/vod/",
    "æé€Ÿ": "https://jszyapi.com/api.php/provide/vod/",
    "è±ªå": "https://hhzyapi.com/api.php/provide/vod/",
    "é‡‘é¹°": "https://jyzyapi.com/api.php/provide/vod/"
}

ACTIVE_SOURCES = {}

# ğŸ’ æµ·é‡å‚ç±»çŸ©é˜µ (æ€»è®¡è¿‘ 30000 éƒ¨)
LIBRARY_CATEGORIES = {
    "ğŸ”¥ å…¨ç½‘æºç«™å®æ—¶çƒ­æ¦œ (å…è±†ç“£)": {"type": "cms_hot"},
    "ğŸ†• æœ€æ–°ä¸Šçº¿ç”µå½±": {"type": "movie", "tag": "æœ€æ–°", "sort": "time"},
    # ã€V6.9 ä¿®å¤ã€‘è±†ç“£TVæ²¡æœ‰"æœ€æ–°"æ ‡ç­¾ï¼Œæ”¹ä¸º"çƒ­é—¨"å¹¶æŒ‰æ—¶é—´æ’åºå³å¯è·å–æœ€æ–°å‰§é›†
    "ğŸ†• æœ€æ–°å¼€æ’­å‰§é›†": {"type": "tv", "tag": "çƒ­é—¨", "sort": "time"},
    "ğŸ¬ çƒ­é—¨ç”µå½±æ€»æ¦œ": {"type": "movie", "tag": "çƒ­é—¨", "sort": "recommend"},
    "ğŸ† è±†ç“£é«˜åˆ†ç¥ä½œ": {"type": "movie", "tag": "è±†ç“£é«˜åˆ†", "sort": "recommend"},
    "ğŸ›¸ ç§‘å¹»å·¨åˆ¶ç²¾é€‰": {"type": "movie", "tag": "ç§‘å¹»", "sort": "recommend"},
    "ğŸ”« åŠ¨ä½œçˆ†ç±³èŠ±æ¦œ": {"type": "movie", "tag": "åŠ¨ä½œ", "sort": "recommend"},
    "ğŸ˜‚ å–œå‰§æ¬¢ä¹ç²¾é€‰": {"type": "movie", "tag": "å–œå‰§", "sort": "recommend"},
    "ğŸ‘» æƒŠæ‚šæ‚¬ç–‘çƒ§è„‘": {"type": "movie", "tag": "æ‚¬ç–‘", "sort": "recommend"},
    "ğŸ“º çƒ­é—¨å›½å‰§å¤§å…¨": {"type": "tv", "tag": "å›½äº§å‰§", "sort": "recommend"},
    "ğŸ“º ç»å…¸ç¾å‰§å¤§ç‰‡": {"type": "tv", "tag": "ç¾å‰§", "sort": "recommend"},
    "ğŸŒ¸ é«˜åˆ†åŠ¨æ¼«æ¼«åŒº": {"type": "tv", "tag": "æ—¥æœ¬åŠ¨ç”»", "sort": "recommend"},
    "ğŸŒ ç»å…¸çºªå½•ç‰‡åº“": {"type": "tv", "tag": "çºªå½•ç‰‡", "sort": "recommend"}
}

DOUBAN_LOCK = threading.Lock()
# ã€V6.9 æ›´æ–°ã€‘ä¿®æ”¹ä¸»ç¼“å­˜æ–‡ä»¶åï¼Œå½»åº•æŠ›å¼ƒä¹‹å‰å› ä¸ºæ ‡ç­¾é”™è¯¯å¯¼è‡´çš„ç©ºè½½æ•°æ®
CACHE_FILE = "douban_cache_v6.json" 
DOUBAN_CACHE = {}
TV_EPISODES_CACHE = {}

if os.path.exists(CACHE_FILE):
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            DOUBAN_CACHE = json.load(f)
    except: pass

def save_cache():
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(DOUBAN_CACHE, f, ensure_ascii=False, indent=2)
    except: pass

def get_random_ua():
    return random.choice([
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/119.0.0.0 Safari/537.36"
    ])

# ==========================================
# ğŸš€ æ¢é’ˆå¼•æ“
# ==========================================
def probe_single_source(name, url):
    try:
        start_time = time.time()
        r = requests.get(f"{url}?ac=list", headers={'User-Agent': get_random_ua()}, timeout=3, verify=False)
        if r.status_code == 200 and 'list' in r.json():
            return name, url, time.time() - start_time
    except: pass
    return name, None, 999

def update_active_sources():
    global ACTIVE_SOURCES
    print("\n[*] ğŸ•µï¸â€â™‚ï¸ æ¢é’ˆå¼•æ“å¯åŠ¨: æ­£åœ¨å…¨ç½‘æ‰«æå­˜æ´»æºç«™...")
    temp_sources = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        futures = [executor.submit(probe_single_source, name, url) for name, url in MASTER_SOURCES.items()]
        for future in concurrent.futures.as_completed(futures):
            name, valid_url, ping = future.result()
            if valid_url: temp_sources[name] = valid_url
                
    if temp_sources:
        ACTIVE_SOURCES = temp_sources
        print(f"[*] âœ… æ¢é’ˆæŠ¥å‘Š: æˆåŠŸç­›é€‰å‡º {len(ACTIVE_SOURCES)} ä¸ªé«˜é€Ÿå­˜æ´»èŠ‚ç‚¹ï¼")
    else:
        ACTIVE_SOURCES = {"é»˜è®¤å…œåº•": "http://cj.ffzyapi.com/api.php/provide/vod/"}

def background_probe_task():
    while True:
        time.sleep(43200)
        update_active_sources()

update_active_sources()
threading.Thread(target=background_probe_task, daemon=True).start()

# ==========================================
# 1. æ•°æ®è·å–å¼•æ“ 
# ==========================================
def fetch_cms_hot_list():
    cache_key = "cms_global_hot_v5" 
    if cache_key in DOUBAN_CACHE: return DOUBAN_CACHE[cache_key]
    
    results = set()
    def fetch_single_cms(url):
        for page in range(1, 6):
            try:
                r = requests.get(f"{url}?ac=detail&pg={page}", headers={'User-Agent': get_random_ua()}, timeout=5, verify=False)
                if r.status_code == 200:
                    v_list = r.json().get('list', [])
                    if not v_list: break 
                    
                    for vod in v_list:
                        name = re.sub(r'[\\/*?:"<>|]', "", vod.get('vod_name', '')).strip()
                        t_name = str(vod.get('type_name', ''))
                        
                        black_keywords = ["è§£è¯´", "é€Ÿçœ‹", "é¢„å‘Š", "åˆ†é’Ÿ", "çŸ­å‰§", "èŠ±çµ®", "ç›˜ç‚¹", "åˆé›†", 
                                          "ç¦åˆ©", "ä¼¦ç†", "ç»¼è‰º", "è®°å½•", "çºªå½•", "åŠ¨æ¼«", "åŠ¨ç”»", "ä½“è‚²", 
                                          "éŸ³ä¹", "å…¶ä»–", "å†™çœŸ", "å¾®ç”µ", "ç›²ç›’", "å¤´æ¡", "Bç«™", "æŠ–éŸ³", "å¿«æ‰‹"]
                        
                        if any(x in name for x in black_keywords) or any(x in t_name for x in black_keywords): 
                            continue
                            
                        white_type_keywords = ["ç‰‡", "ç”µå½±", "å‰§", "è¿ç»­å‰§", "TV"]
                        if not any(x in t_name for x in white_type_keywords):
                            continue 
                            
                        results.add(name)
            except: pass

    print(f"\n[*] ğŸš€ æ­£åœ¨ç©¿é€ {len(ACTIVE_SOURCES)} ä¸ªå­˜æ´»æºç«™ï¼Œå‘ä¸‹æ·±æŒ– 5 é¡µè¿›è¡Œå…¨ç½‘æµ·é€‰...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(ACTIVE_SOURCES)) as executor:
        [executor.submit(fetch_single_cms, url) for url in ACTIVE_SOURCES.values()]
    
    final_list = list(results)
    if len(final_list) > 800: final_list = final_list[:800]
        
    if final_list:
        DOUBAN_CACHE[cache_key] = final_list
        save_cache()
        
    print(f"[*] âœ… å…è±†ç“£çƒ­æ¦œæå–å®Œæ¯•ï¼æœ€ç»ˆæ–©è· {len(final_list)} éƒ¨çº¯å‡€å¤§ç‰‡/çƒ­å‰§ï¼")
    return final_list

def fetch_douban_chunk(tag, is_movie, offset=0, count=250, sort_method="recommend"):
    t_type = "movie" if is_movie else "tv"
    cache_key = f"{t_type}_{tag}_{sort_method}_{offset}_{count}"
    
    if cache_key in DOUBAN_CACHE: return DOUBAN_CACHE[cache_key]

    with DOUBAN_LOCK:
        if cache_key in DOUBAN_CACHE: return DOUBAN_CACHE[cache_key]
        
        urls = [f"https://movie.douban.com/j/search_subjects?type={t_type}&tag={urllib.parse.quote(tag)}&sort={sort_method}&page_limit=50&page_start={i}" for i in range(offset, offset + count, 50)]
        results, seen = [], set()
        
        print(f"\n[*] ğŸŒŠ [åå°å»ºåº“] æ­£åœ¨æŠ“å–è±†ç“£ {t_type} ç±»çš„ {tag} (ç¬¬{offset+1}åˆ°{offset+count}éƒ¨) ...")
        for url in urls:
            try:
                # ã€V6.9 æ ¸å¿ƒä¼˜åŒ–ã€‘å»¶æ—¶å‹ç¼©è‡³ 0.4-0.8 ç§’ï¼Œæ€»è€—æ—¶æ§åˆ¶åœ¨ 4 ç§’å†…ï¼Œé˜²æ­¢æ’­æ”¾å™¨ PROPFIND è¶…æ—¶æ–­è¿
                time.sleep(random.uniform(0.4, 0.8))
                r = requests.get(url, headers={'User-Agent': get_random_ua(), 'Referer': 'https://movie.douban.com/'}, timeout=8)
                if r.status_code == 403: 
                    print(" [!] è§¦å‘é™æµï¼Œé¿é™©ä¸­...")
                    return ["è±†ç“£æ¥å£é™åˆ¶_è¯·ä½¿ç”¨å…è±†ç“£æˆ–ç¨ååˆ·æ–°"]
                if r.status_code == 200:
                    for i in r.json().get('subjects', []):
                        name = re.sub(r'[\\/*?:"<>|]', "", i.get('title', '')).strip()
                        if name and name not in seen:
                            seen.add(name); results.append(name)
            except: pass
            
        if results:
            DOUBAN_CACHE[cache_key] = results
            save_cache()
            return results
            
        # å¦‚æœè±†ç“£ç¡®å®æ²¡æœ‰è¿”å›æ•°æ®ï¼ˆä¾‹å¦‚éæ³•tagç»„åˆï¼‰
        print(" [!] è­¦å‘Šï¼šè±†ç“£ API æœªè¿”å›ä»»ä½•æ•°æ®ã€‚")
        return ["è¯¥åˆ†ç±»è±†ç“£æš‚æ— æ•°æ®_æˆ–æ¥å£å¼‚å¸¸"]

# ==========================================
# 2. æ´—æµä¸æœç´¢å¼•æ“
# ==========================================
def clean_m3u8_stream(m3u8_url):
    try:
        r = requests.get(m3u8_url, headers={'User-Agent': get_random_ua()}, timeout=8, verify=False)
        content = r.text
        if "RESOLUTION=" in content:
            for line in content.splitlines():
                if line.endswith('.m3u8'):
                    return clean_m3u8_stream(line if line.startswith('http') else f"{m3u8_url.rsplit('/', 1)[0]}/{line}")
                    
        lines = content.splitlines()
        base_path = m3u8_url.rsplit('/', 1)[0]
        ts_urls = [line if line.startswith('http') else f"{base_path}/{line}" for line in lines if not line.startswith('#') and line.strip()]
        if not ts_urls: return content
        
        main_domain = Counter([urllib.parse.urlparse(u).netloc for u in ts_urls]).most_common(1)[0][0]
        clean_lines, has_vod_tag, i = [], False, 0
        
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1; continue
            if line.startswith('#EXT-X-PLAYLIST-TYPE'):
                has_vod_tag = True; clean_lines.append(line)
            elif line.startswith('#EXTINF'):
                extinf_line = line
                i += 1
                if i < len(lines):
                    ts_url = lines[i].strip()
                    ts_url = ts_url if ts_url.startswith('http') else f"{base_path}/{ts_url}"
                    if urllib.parse.urlparse(ts_url).netloc == main_domain:
                        clean_lines.extend([extinf_line, ts_url])
            elif line.startswith('#EXTM3U') or line.startswith('#EXT-X-VERSION') or line.startswith('#EXT-X-TARGETDURATION') or line.startswith('#EXT-X-MEDIA-SEQUENCE'):
                clean_lines.append(line)
            i += 1

        if not has_vod_tag and len(clean_lines) > 1: clean_lines.insert(1, "#EXT-X-PLAYLIST-TYPE:VOD")
        clean_lines.append("#EXT-X-ENDLIST")
        return '\n'.join(clean_lines)
    except: return None

def check_playability_and_duration(m3u8_url):
    try:
        r = requests.get(m3u8_url, headers={'User-Agent': get_random_ua()}, timeout=5, verify=False)
        if r.status_code != 200: return False
        content = r.text
        if "RESOLUTION=" in content: return True
        return sum(float(m) for m in re.findall(r'#EXTINF:([\d\.]+)', content)) >= 1800
    except: return False

@app.route('/proxy/m3u8')
def proxy_m3u8():
    url = request.args.get('url')
    cleaned = clean_m3u8_stream(url)
    if cleaned: return Response(cleaned, mimetype='application/vnd.apple.mpegurl')
    return redirect(url, code=302)

def search_single_api(api_url, keyword):
    try:
        r = requests.get(f"{api_url}?ac=detail&wd={urllib.parse.quote(keyword)}", headers={'User-Agent': get_random_ua()}, timeout=6, verify=False)
        valid_vods = []
        for vod in r.json().get('list', []):
            name, t_name = vod.get('vod_name', ''), str(vod.get('type_name', ''))
            black_keywords = ["è§£è¯´", "é€Ÿçœ‹", "é¢„å‘Š", "åˆ†é’Ÿ", "çŸ­å‰§", "èŠ±çµ®", "ç›˜ç‚¹", "åˆé›†", "ç¦åˆ©", "ä¼¦ç†", "ç»¼è‰º", "è®°å½•", "åŠ¨æ¼«"]
            if any(x in name for x in black_keywords) or any(x in t_name for x in black_keywords): continue
            valid_vods.append(vod)
        return valid_vods
    except: return []

def get_movie_stream(keyword):
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(ACTIVE_SOURCES)) as executor:
        futures = [executor.submit(search_single_api, url, keyword) for url in ACTIVE_SOURCES.values()]
        for future in concurrent.futures.as_completed(futures):
            for vod in future.result():
                if keyword not in vod.get('vod_name', ''): continue
                for group in vod.get('vod_play_url', '').split('$$$'):
                    if '.m3u8' in group or '.mp4' in group:
                        for ep in group.split('#'):
                            ep_url = ep.split('$', 1)[1] if '$' in ep else ep
                            if check_playability_and_duration(ep_url): return ep_url
    return None

def get_tv_episodes(keyword):
    episodes_dict = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(ACTIVE_SOURCES)) as executor:
        futures = {executor.submit(search_single_api, url, keyword): name for name, url in ACTIVE_SOURCES.items()}
        for future in concurrent.futures.as_completed(futures):
            source_name = futures[future]
            for vod in future.result():
                if keyword not in vod.get('vod_name', ''): continue
                for group in vod.get('vod_play_url', '').split('$$$'):
                    if '.m3u8' in group or '.mp4' in group:
                        for ep in group.split('#'):
                            if '$' in ep:
                                ep_name, ep_url = ep.split('$', 1)
                                safe_ep_name = re.sub(r'[\\/*?:"<>|]', "", ep_name).strip()
                                episodes_dict[f"[{source_name}] {keyword}_{safe_ep_name}.mp4"] = ep_url
                        return episodes_dict 
    return episodes_dict

# ==========================================
# 3. WebDAV è·¯ç”± (æµ·é‡æ·±æ½œæ¶æ„)
# ==========================================
def generate_propfind_xml(items):
    xml = ['<?xml version="1.0" encoding="utf-8" ?>', '<D:multistatus xmlns:D="DAV:">']
    for item in items:
        item_path = urllib.parse.quote(item['path'])
        xml.append(f'  <D:response>\n    <D:href>{item_path}</D:href>\n    <D:propstat><D:prop>')
        xml.append(f'      <D:displayname>{item["name"]}</D:displayname>')
        if item['is_dir']: xml.append('      <D:resourcetype><D:collection/></D:resourcetype>')
        else:
            xml.append('      <D:resourcetype/>\n      <D:getcontentlength>1073741824</D:getcontentlength>\n      <D:getcontenttype>video/mp4</D:getcontenttype>')
        xml.append('      <D:getlastmodified>Tue, 10 Jan 2024 12:00:00 GMT</D:getlastmodified>\n    </D:prop></D:propstat>\n    <D:status>HTTP/1.1 200 OK</D:status>\n  </D:response>')
    xml.append('</D:multistatus>')
    return '\n'.join(xml)

@app.route('/', defaults={'path': ''}, methods=['OPTIONS', 'PROPFIND', 'GET', 'HEAD'])
@app.route('/<path:path>', methods=['OPTIONS', 'PROPFIND', 'GET', 'HEAD'])
def webdav_handler(path):
    full_path = '/' + path if path else '/'
    decoded_path = urllib.parse.unquote(full_path).rstrip('/')
    parts = [p for p in decoded_path.split('/') if p]

    if request.method == 'OPTIONS':
        resp = Response()
        resp.headers['Allow'] = 'OPTIONS, PROPFIND, GET, HEAD'
        resp.headers['DAV'] = '1, 2'
        return resp

    if request.method == 'PROPFIND':
        items = []
        depth = request.headers.get('Depth', '1')

        # æ ¹ç›®å½•ï¼šå±•ç¤ºåå‡ å¤§åˆ†ç±»
        if len(parts) == 0:
            items.append({'path': '/', 'name': 'Root', 'is_dir': True})
            if depth != '0':
                for cat in LIBRARY_CATEGORIES.keys(): items.append({'path': f"/{cat}", 'name': cat, 'is_dir': True})

        # ä¸€çº§ç›®å½•ï¼šç”Ÿæˆ 10 ä¸ªæ·±åº¦åˆ†é¡µæ–‡ä»¶å¤¹
        elif len(parts) == 1 and parts[0] in LIBRARY_CATEGORIES:
            items.append({'path': decoded_path, 'name': parts[0], 'is_dir': True})
            if depth != '0':
                cat_config = LIBRARY_CATEGORIES[parts[0]]
                if cat_config['type'] == 'cms_hot':
                    for name in fetch_cms_hot_list(): items.append({'path': f"{decoded_path}/{name}", 'name': name, 'is_dir': True})
                else:
                    for i in range(10):
                        start_idx = i * 250 + 1
                        end_idx = (i + 1) * 250
                        prefix = "ğŸ”¥" if i == 0 else "ğŸ“š"
                        folder_name = f"{prefix} Top {start_idx}-{end_idx}"
                        items.append({'path': f"{decoded_path}/{folder_name}", 'name': folder_name, 'is_dir': True})

        # äºŒçº§ç›®å½•ï¼šæå–æ•°æ®
        elif len(parts) == 2 and parts[0] in LIBRARY_CATEGORIES:
            items.append({'path': decoded_path, 'name': parts[1], 'is_dir': True})
            if depth != '0':
                cat_config = LIBRARY_CATEGORIES[parts[0]]
                if cat_config['type'] == 'cms_hot':
                    name = parts[1]
                    if name not in TV_EPISODES_CACHE: TV_EPISODES_CACHE[name] = get_tv_episodes(name)
                    episodes = TV_EPISODES_CACHE[name]
                    if not episodes: items.append({'path': f"{decoded_path}/æœªæ‰¾åˆ°æœ‰æ•ˆæº.mp4", 'name': "æœªæ‰¾åˆ°æœ‰æ•ˆæº.mp4", 'is_dir': False})
                    else:
                        for ep_name in episodes.keys(): items.append({'path': f"{decoded_path}/{ep_name}", 'name': ep_name, 'is_dir': False})
                else:
                    match = re.search(r'Top (\d+)-', parts[1])
                    offset = int(match.group(1)) - 1 if match else 0
                    
                    for name in fetch_douban_chunk(cat_config['tag'], (cat_config['type'] == 'movie'), offset=offset, count=250, sort_method=cat_config['sort']):
                        if cat_config['type'] == 'movie': items.append({'path': f"{decoded_path}/{name}.mp4", 'name': f"{name}.mp4", 'is_dir': False})
                        else: items.append({'path': f"{decoded_path}/{name}", 'name': name, 'is_dir': True})

        # ä¸‰çº§ç›®å½•ï¼šå¦‚æœæ˜¯ç”µè§†å‰§ï¼Œç‚¹è¿›å»å±•å¼€é›†æ•°
        elif len(parts) == 3:
            if decoded_path.endswith('.mp4'):
                items.append({'path': decoded_path, 'name': parts[-1], 'is_dir': False})
            else:
                tv_name = parts[-1]
                items.append({'path': decoded_path, 'name': tv_name, 'is_dir': True})
                if depth != '0':
                    if tv_name not in TV_EPISODES_CACHE: TV_EPISODES_CACHE[tv_name] = get_tv_episodes(tv_name)
                    episodes = TV_EPISODES_CACHE[tv_name]
                    if not episodes: items.append({'path': f"{decoded_path}/æœªæ‰¾åˆ°è¯¥æºæˆ–æ—¶é•¿è¿‡çŸ­.mp4", 'name': "æœªæ‰¾åˆ°è¯¥æºæˆ–æ—¶é•¿è¿‡çŸ­.mp4", 'is_dir': False})
                    else:
                        for ep_name in episodes.keys(): items.append({'path': f"{decoded_path}/{ep_name}", 'name': ep_name, 'is_dir': False})

        elif len(parts) == 4 and decoded_path.endswith('.mp4'):
            items.append({'path': decoded_path, 'name': parts[-1], 'is_dir': False})
        else: return Response("Not Found", status=404)

        return Response(generate_propfind_xml(items), status=207, mimetype='application/xml')

    if request.method in ['GET', 'HEAD']:
        if decoded_path.endswith('.mp4'):
            parent_dir = parts[-2]
            file_name = parts[-1]
            m3u8_url = TV_EPISODES_CACHE.get(parent_dir, {}).get(file_name)
            
            if not m3u8_url:
                movie_name = file_name.replace('.mp4', '')
                m3u8_url = get_movie_stream(movie_name)
                
            if m3u8_url: return redirect(f"/proxy/m3u8?url={urllib.parse.quote(m3u8_url)}", code=302) 

    return Response("Method Not Allowed", status=405)

if __name__ == '__main__':
    print("="*75)
    print(f" ğŸŒ WebDAV å½±è§†ç»ˆæå¼•æ“ V6.9 (æœ€ç»ˆå®Œå–„ç‰ˆ) å¯åŠ¨å°±ç»ªï¼")
    print("="*75)
    app.run(host='0.0.0.0', port=8080, debug=False)
